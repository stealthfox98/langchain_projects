{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPFblz6d749b"
      },
      "source": [
        "# Build End-to-End LLM project for a retail domain (t-shirts selling store)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fDCRsNP7xEv"
      },
      "source": [
        "Tech stack for this project\n",
        "* UI: Streamlit\n",
        "* LLM: Google Gemini LLM model\n",
        "* Embeddings: Hugging Face\n",
        "* Framework: Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
        "from langchain import hub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Insert API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initializing Gemini AI model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSKhzSaW7o_c"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Establishing database connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_user = \"root\"\n",
        "db_password = \"Kitsune#sql98\"\n",
        "db_host = \"localhost\"\n",
        "db_name = \"bank_data\"\n",
        "\n",
        "# Name of test databases\n",
        "# \"atliq_tshirts\"\n",
        "# \"bank_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_Uri = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
        "db = SQLDatabase.from_uri(connection_Uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(db.table_info)\n",
        "print(\"--------\")\n",
        "print(\"--------\")\n",
        "print(f\"Dialect: {db.dialect}\")\n",
        "print(f\"Available tables: {db.get_usable_table_names()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Core functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SQL Query Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Pulls a specialized SQL generation prompt from LangChain Hub\n",
        "2. Formats the prompt with database schema information\n",
        "3. Sends the prompt to Gemini AI for query generation\n",
        "4. Uses a second prompt to extract just the SQL from the response\n",
        "\"\"\"\n",
        "\n",
        "query_prompt_template = hub.pull(\"langchain-ai/sql-query-system-prompt\")\n",
        "\n",
        "def write_query(question: str):\n",
        "    \"\"\"Generate SQL query from the user's question.\"\"\"\n",
        "    prompt = query_prompt_template.invoke(\n",
        "        {\n",
        "            \"dialect\": db.dialect,\n",
        "            \"top_k\": 10,\n",
        "            \"table_info\": db.get_table_info(),\n",
        "            \"input\": question,\n",
        "        }\n",
        "    )\n",
        "    response = llm.invoke(prompt.to_string())\n",
        "    extraction_prompt = \"\"\"\n",
        "    Please extract the SQL query from the following text and return only the SQL query without any additional characters or formatting:\n",
        "\n",
        "    {response}\n",
        "\n",
        "    SQL Query:\n",
        "    \"\"\"\n",
        "    # Format the prompt with the actual response\n",
        "    prompt = extraction_prompt.format(response=response)\n",
        "    # Invoke the language model with the prompt\n",
        "    parsed_query = llm.invoke(prompt)\n",
        "    return parsed_query.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function creates a query execution tool and runs the generated SQL against your database, returning the raw results.\n",
        "\"\"\"\n",
        "\n",
        "def execute_query(query: str):\n",
        "    \"\"\"Execute the SQL query.\"\"\"\n",
        "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
        "    return execute_query_tool.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Natural Language Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function takes the original question, generated SQL, and query results, then asks Gemini to formulate a human-friendly answer.\n",
        "\"\"\"\n",
        "\n",
        "def generate_answer(question: str, query: str, result: str):\n",
        "    \"\"\"Generate an answer using the query results.\"\"\"\n",
        "    prompt = (\n",
        "        \"Given the following user question, corresponding SQL query, \"\n",
        "        \"and SQL result, answer the user question.\\n\\n\"\n",
        "        f'Question: {question}\\n'\n",
        "        f'SQL Query: {query}\\n'\n",
        "        f'SQL Result: {result}'\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting It All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_output(input):\n",
        "    query = write_query(input)\n",
        "    result = execute_query(query) # type: ignore\n",
        "    answer = generate_answer(input, query, result) # type: ignore\n",
        "\n",
        "    print(f\"Query: \\n\\n{query}\")\n",
        "    print(\"\\n\")\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Questions for the \"altiq_tshirts\" database\n",
        "\"\"\"\n",
        "\"How many t-shirts do we have left for nike in extra small size and white color?\"\n",
        "\"How much is the price of the inventory for all small size t-shirts?\"\n",
        "\"If we have to sell all the Levi’s T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\"\n",
        "\"If we have to sell all the Van Heuson T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\"\n",
        "\"\"\"\n",
        "\n",
        "# question = string_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Questions for the \"bank_data\" database\n",
        "\"\"\"\n",
        "\"What is the total balance of all the accounts in the database?\"\n",
        "\"Which account has the lowest balance and what is the client name?\"\n",
        "\"Name of the client who has the highest credit card limit and what is the limit\"\n",
        "\"Name of the client with most number of credit cards, how many and total limit.\"\n",
        "\"Name of the client with the second top number of credit cards, how many and total limit.\"\n",
        "\"\"\"\n",
        "\n",
        "question = \"Proporciona el nombre y código de clientes con ahorros mayores a cero y su cantidad de tarjetas de crédito. Ordénalos de acuerdo con su número de tarjetas y muestra los resultados en formato tabular.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_output(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
